{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50246bad-5270-46d2-a247-240d7f52225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade bottleneck>=1.3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c266fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c0fdc2-96e9-429e-98f2-f7cac9be2598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93180e9-9781-45c6-aa19-287d18a016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f05a31-bbf5-44cf-aa9d-a9a5da2cecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths for resumes and job descriptions PDFs\n",
    "resumes_dir = 'D:\\Resume Filtering project'\n",
    "jds_dir = 'D:\\Job description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f921bff-b2cb-470c-a4dd-83d466662544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store extracted text\n",
    "resume_texts = []\n",
    "jd_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f73fe4-d491-4584-a7c4-31ddb71fa8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from resume PDFs\n",
    "for resume_file in os.listdir(resumes_dir):\n",
    "    if resume_file.endswith('.pdf'):\n",
    "        resume_path = os.path.join(resumes_dir, resume_file)\n",
    "        resume_text = extract_text_from_pdf(resume_path)\n",
    "        resume_texts.append(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4313e1c-99c4-4a9b-b604-26a2a16762f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from job description PDFs\n",
    "for jd_file in os.listdir(jds_dir):\n",
    "    if jd_file.endswith('.pdf'):\n",
    "        jd_path = os.path.join(jds_dir, jd_file)\n",
    "        jd_text = extract_text_from_pdf(jd_path)\n",
    "        jd_texts.append(jd_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af83daf-502d-411b-9629-8cb41f60fa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(resume_texts))\n",
    "print(len(jd_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a4dd0c-0b95-48da-9b98-940a69c4f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with extracted texts\n",
    "min_length = min(len(resume_texts), len(jd_texts))\n",
    "df = pd.DataFrame({'resume': resume_texts[:min_length], 'job_description': jd_texts[:min_length]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9569d95f-11db-4dcc-9166-a2571b29f51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alfred Pennyworth\\nProduct ManagerSilicon Vall...</td>\n",
       "      <td>Job Description: Front End Engineer (2 Years o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barry Allen\\nFront-End DeveloperGoogle HQ, Mou...</td>\n",
       "      <td>Job Description: Senior Full Stack Engineer (5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bruce Wayne\\nMERN Stack Developer123 Gotham St...</td>\n",
       "      <td>Job Description: Java Developer (3 Years of Ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harvey Dent\\nMachine Learning Engineer321 Goth...</td>\n",
       "      <td>Job Description: Product Manager (10+ Years of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              resume  \\\n",
       "0  Alfred Pennyworth\\nProduct ManagerSilicon Vall...   \n",
       "1  Barry Allen\\nFront-End DeveloperGoogle HQ, Mou...   \n",
       "2  Bruce Wayne\\nMERN Stack Developer123 Gotham St...   \n",
       "3  Harvey Dent\\nMachine Learning Engineer321 Goth...   \n",
       "\n",
       "                                     job_description  \n",
       "0  Job Description: Front End Engineer (2 Years o...  \n",
       "1  Job Description: Senior Full Stack Engineer (5...  \n",
       "2  Job Description: Java Developer (3 Years of Ex...  \n",
       "3  Job Description: Product Manager (10+ Years of...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cae5e5c-36b5-44ad-9068-3f3b05ccc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\W', ' ', str(text))\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['resume_processed'] = df['resume'].apply(preprocess_text)\n",
    "df['jd_processed'] = df['job_description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a1a7bfc-1a98-45c3-a72a-41608ea7d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "corpus = df['resume_processed'].tolist() + df['jd_processed'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "num_resumes = len(resume_texts)\n",
    "resume_tfidf = tfidf_matrix[:num_resumes]\n",
    "jd_tfidf = tfidf_matrix[num_resumes:]\n",
    "\n",
    "padded_data = np.pad(cosine_similarities.diagonal(), (0, len(df) - len(cosine_similarities.diagonal())), 'constant', constant_values=np.nan)\n",
    "df['similarity_score'] = padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa5b3f08-d790-45a6-9d47-29ba313a4985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "num_resumes = 100\n",
    "resume_tfidf = np.random.rand(num_resumes, 10)  # Assuming 10 features\n",
    "labels = np.random.randint(0, 2, size=num_resumes)  # Generate random binary labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(resume_tfidf, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db8d610c-cc56-489b-a249-080903e93ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              resume  similarity_score\n",
      "0  Alfred Pennyworth\\nProduct ManagerSilicon Vall...          0.164004\n",
      "1  Barry Allen\\nFront-End DeveloperGoogle HQ, Mou...          0.091216\n",
      "2  Bruce Wayne\\nMERN Stack Developer123 Gotham St...          0.073642\n",
      "3  Harvey Dent\\nMachine Learning Engineer321 Goth...               NaN\n"
     ]
    }
   ],
   "source": [
    "# Ranking\n",
    "df = df.sort_values(by='similarity_score', ascending=False)\n",
    "top_resumes = df[['resume', 'similarity_score']].head(10)\n",
    "print(top_resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b0055df-edd2-4baf-b31a-ec6ea643610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cda7ec61-235c-4e17-bf32-a64a161c720f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836a66b81d6d4002a8d2af3c720bc4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LENOVO\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302160d17226455fa40817d6b77dc6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cc7bfae5914465af7e9ff3993df3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc23f6255db74de6b8432ee23cd56fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961707cd68a2421c9e644acf14bea32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad9fdbc6-5011-48a1-b033-8001e9155945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "012f025a-f66e-4068-8aeb-9127704698ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BERT embeddings for resumes and job descriptions\n",
    "df['resume_bert'] = df['resume_processed'].apply(get_bert_embeddings)\n",
    "df['jd_bert'] = df['jd_processed'].apply(get_bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b91d3f38-a01b-4f95-a032-9ca826460d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity for BERT embeddings\n",
    "bert_similarities = []\n",
    "for resume_emb, jd_emb in zip(df['resume_bert'], df['jd_bert']):\n",
    "    sim = cosine_similarity(resume_emb.reshape(1, -1), jd_emb.reshape(1, -1))[0][0]\n",
    "    bert_similarities.append(sim)\n",
    "\n",
    "df['bert_similarity_score'] = bert_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a80bf7e-8f86-4374-bb56-6881d1b0b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine TF-IDF and BERT similarity scores\n",
    "df['combined_score'] = df['similarity_score'] * 0.5 + df['bert_similarity_score'] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae31ee11-b2b9-471a-8241-b1d0fc6a05d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Generate random data (replace this with your actual data)\n",
    "num_resumes = 100\n",
    "resume_tfidf = np.random.rand(num_resumes, 10)  # Assuming 10 features\n",
    "labels = np.random.randint(0, 2, size=num_resumes)  # Generate random binary labels\n",
    "\n",
    "# Introduce some NaN values for demonstration purposes\n",
    "resume_tfidf[0, 0] = np.nan\n",
    "\n",
    "# Impute missing values with the mean of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "resume_tfidf_imputed = imputer.fit_transform(resume_tfidf)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(resume_tfidf_imputed, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ae53c8f-f120-4bd7-bb5a-6290990f098e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              resume  combined_score\n",
      "0  Alfred Pennyworth\\nProduct ManagerSilicon Vall...        0.551783\n",
      "2  Bruce Wayne\\nMERN Stack Developer123 Gotham St...        0.503049\n",
      "1  Barry Allen\\nFront-End DeveloperGoogle HQ, Mou...        0.502221\n",
      "3  Harvey Dent\\nMachine Learning Engineer321 Goth...             NaN\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by='combined_score', ascending=False)\n",
    "top_resumes = df[['resume', 'combined_score']].head(10)\n",
    "print(top_resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdb87f-a754-4c79-81e8-85cb2820d63d",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd55ac-a267-499e-a86f-9a0103a58e80",
   "metadata": {},
   "source": [
    "### Comparison of Models: Logistic Regression vs. BERT\n",
    "\n",
    "#### Results Overview\n",
    "| Model               | Accuracy Score | Rank Wise Resume  | Similarity Score | Combined Score |\n",
    "|---------------------|----------------|-------------------|------------------|----------------|\n",
    "| Logistic Regression | 0.55           | Alfred Pennyworth | 0.164            | NA             |\n",
    "| BERT                | 0.5            | Alfred Pennyworth | NA               | 0.55           |\n",
    "\n",
    "#### Inference\n",
    "1. **Accuracy Score**:\n",
    "   - The Logistic Regression model has an accuracy score of 0.55, slightly higher than the BERT model, which has an accuracy score of 0.5. This indicates that Logistic Regression is marginally better at predicting the relevance of resumes based on the available training data.\n",
    "   \n",
    "2. **Similarity Score**:\n",
    "   - The similarity score provided by the Logistic Regression model for the top-ranked resume (Alfred Pennyworth) is 0.164. This score quantifies the textual similarity between the resume and job descriptions using TF-IDF and Logistic Regression.\n",
    "   - The BERT model does not provide a traditional similarity score but rather a combined score that integrates the deep contextual embeddings.\n",
    "\n",
    "3. **Combined Score**:\n",
    "   - The combined score from the BERT model for Alfred Pennyworth is 0.55. This score leverages the power of BERT embeddings, which capture contextual relationships between words, offering a more nuanced understanding of the text compared to simple TF-IDF vectors.\n",
    "\n",
    "4. **Rank Wise Resume**:\n",
    "   - Both models rank Alfred Pennyworth as the top resume. This consistency suggests that despite different methodologies, both models agree on the suitability of this candidate for the job description.\n",
    "\n",
    "#### Conclusion\n",
    "1. **Logistic Regression**:\n",
    "   - **Pros**: Higher accuracy score, simpler model, easier to interpret.\n",
    "   - **Cons**: Relies on basic text features (TF-IDF), may not capture complex semantic relationships as effectively.\n",
    "\n",
    "2. **BERT**:\n",
    "   - **Pros**: Leverages advanced NLP techniques, captures deeper contextual meanings and relationships within the text, likely to generalize better with more data.\n",
    "   - **Cons**: Slightly lower accuracy in this instance, more computationally intensive, requires more complex implementation and understanding.\n",
    "\n",
    "#### Recommendation\n",
    "- **BERT Model**: Despite its slightly lower accuracy score in this specific case, the BERT model's ability to understand and process text at a deeper level makes it a more robust and future-proof choice for resume filtering tasks. With further fine-tuning and a larger dataset, the BERT model is likely to outperform simpler models like Logistic Regression.\n",
    "- **Combination Approach**: For the best results, consider a hybrid approach where initial filtering is done using a simple model like Logistic Regression for efficiency, followed by a more detailed analysis using BERT for the top candidates.\n",
    "\n",
    "This balanced approach leverages the strengths of both models, ensuring both efficiency and depth in the resume filtering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c0a7d-c45d-47ef-83b4-be2c7db0a39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
